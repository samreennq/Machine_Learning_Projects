{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News Category Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2222</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "2220       business  cars pull down us retail figures us retail sal...\n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
       "2223       politics  how political squabbles snowball it s become c...\n",
       "2224          sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of BBC news stories and their classification into categories. There are 2225 observations. There are five categories in the target variable of this dataset. Out of 2225 categories the most are sport (511) and politics (510) and the fewest of the news fall into the category of entertainment (386)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps I need to take while preprocessing this data.\n",
    "\n",
    "1) The independet variable, the actual news stories, are all cut after the first 100 words and the top 10,000 words are tokenized.\n",
    "\n",
    "2) The dependent variable needs to be either one hot encoded or be obtained the dummies for. I prefer to use pandas to get dummies rather than using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text']\n",
    "\n",
    "category=data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29726 unique tokens.\n",
      "Shape of data tensor: (2225, 100)\n",
      "Shape of category tensor: (2225,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "max_words = 10000  \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "text = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "category = np.asarray(category)\n",
    "\n",
    "print('Shape of data tensor:',text.shape)\n",
    "print('Shape of category tensor:',category.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      business  entertainment  politics  sport  tech\n",
       "0            0              0         0      0     1\n",
       "1            1              0         0      0     0\n",
       "2            0              0         0      1     0\n",
       "3            0              0         0      1     0\n",
       "4            0              1         0      0     0\n",
       "...        ...            ...       ...    ...   ...\n",
       "2220         1              0         0      0     0\n",
       "2221         0              0         1      0     0\n",
       "2222         0              1         0      0     0\n",
       "2223         0              0         1      0     0\n",
       "2224         0              0         0      1     0\n",
       "\n",
       "[2225 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=text\n",
    "X\n",
    "y=category\n",
    "y\n",
    "\n",
    "y=pd.get_dummies(y)\n",
    "\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: A Model with an embedding layer and two dense layers (with no layers meant for sequential data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668, 100)\n",
      "(1668, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 4005      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 84,035\n",
      "Trainable params: 84,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1668/1668 [==============================] - 0s 250us/step - loss: 0.6696 - acc: 0.5253\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 0s 125us/step - loss: 0.5806 - acc: 0.6897\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 0s 134us/step - loss: 0.5504 - acc: 0.7379\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 0s 123us/step - loss: 0.5293 - acc: 0.8000\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 0s 112us/step - loss: 0.5055 - acc: 0.8008\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 0s 106us/step - loss: 0.4795 - acc: 0.8089\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 0s 110us/step - loss: 0.4542 - acc: 0.8183\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 0s 93us/step - loss: 0.4326 - acc: 0.8237\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 0s 100us/step - loss: 0.4144 - acc: 0.8266\n",
      "Epoch 10/10\n",
      "1668/1668 [==============================] - 0s 94us/step - loss: 0.3992 - acc: 0.8291\n",
      "557/557 [==============================] - 0s 132us/step\n",
      "Test score: 0.41835008617791497\n",
      "Test accuracy: 0.8157988786697388\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, I used embedding and two dense layers to run the model A. The accuracy of the model is 81.58%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: A model using an Embedding layer with Conv1d Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 94, 32)            28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,316,069\n",
      "Trainable params: 1,316,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 1s 1ms/step - loss: 1.8814 - acc: 0.8000 - val_loss: 1.7397 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 1s 848us/step - loss: 1.6143 - acc: 0.8000 - val_loss: 1.7158 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 1s 815us/step - loss: 1.5890 - acc: 0.8000 - val_loss: 1.7015 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 1s 817us/step - loss: 1.5723 - acc: 0.8000 - val_loss: 1.6912 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 1s 799us/step - loss: 1.5596 - acc: 0.8000 - val_loss: 1.6881 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 1s 799us/step - loss: 1.5496 - acc: 0.8000 - val_loss: 1.6823 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 1s 831us/step - loss: 1.5417 - acc: 0.8000 - val_loss: 1.6790 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 1s 831us/step - loss: 1.5349 - acc: 0.8000 - val_loss: 1.6712 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 1s 834us/step - loss: 1.5287 - acc: 0.8000 - val_loss: 1.6684 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 1s 786us/step - loss: 1.5229 - acc: 0.8000 - val_loss: 1.6674 - val_acc: 0.8000\n",
      "557/557 [==============================] - 0s 260us/step\n",
      "Test score: 1.527776182874957\n",
      "Test accuracy: 0.8000001311302185\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "max_features = 10000  \n",
    "max_len = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model.add(layers.MaxPooling1D(5)) #\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(5))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model with embedding and two Conv1D layers yeilds 80% accuracy in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: A model using an Embedding layer with one LSTM sequential layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 6s 4ms/step - loss: 0.5368 - acc: 0.7933 - val_loss: 0.4998 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.4971 - acc: 0.8000 - val_loss: 0.4930 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.4818 - acc: 0.8000 - val_loss: 0.4700 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.4366 - acc: 0.8127 - val_loss: 0.4298 - val_acc: 0.8287\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.3810 - acc: 0.8339 - val_loss: 0.3821 - val_acc: 0.8341\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.3321 - acc: 0.8387 - val_loss: 0.3333 - val_acc: 0.8461\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 5s 4ms/step - loss: 0.2935 - acc: 0.8856 - val_loss: 0.3198 - val_acc: 0.8707\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.2524 - acc: 0.9244 - val_loss: 0.2679 - val_acc: 0.9126\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.2079 - acc: 0.9478 - val_loss: 0.2475 - val_acc: 0.9198\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 5s 3ms/step - loss: 0.1894 - acc: 0.9582 - val_loss: 0.2587 - val_acc: 0.9174\n",
      "557/557 [==============================] - 0s 803us/step\n",
      "Test score: 0.2923532754878381\n",
      "Test accuracy: 0.8987432718276978\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense\n",
    "model_c = Sequential()\n",
    "model_c.add(Embedding(10000, 32))\n",
    "model_c.add(LSTM(32))\n",
    "model_c.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model_c.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model_c.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model_c.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model with a single LSTM model yeilds 90% accuracy. Which is substantially higher than the previous two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: A model using an Embedding layer with stacked LSTM sequential layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 13s 10ms/step - loss: 0.5209 - acc: 0.7925 - val_loss: 0.4979 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.4530 - acc: 0.8136 - val_loss: 0.3903 - val_acc: 0.8347\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.3828 - acc: 0.8319 - val_loss: 0.3529 - val_acc: 0.8389\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 10s 8ms/step - loss: 0.3359 - acc: 0.8499 - val_loss: 0.3505 - val_acc: 0.8449\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.3044 - acc: 0.8663 - val_loss: 0.3876 - val_acc: 0.8383\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.2858 - acc: 0.8711 - val_loss: 0.3408 - val_acc: 0.8431\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 10s 7ms/step - loss: 0.2644 - acc: 0.8780 - val_loss: 0.4208 - val_acc: 0.8168\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.2718 - acc: 0.8741 - val_loss: 0.3214 - val_acc: 0.8569\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.2539 - acc: 0.8750 - val_loss: 0.3179 - val_acc: 0.8569\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 11s 8ms/step - loss: 0.2372 - acc: 0.8787 - val_loss: 0.4214 - val_acc: 0.8449\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "Test score: 0.45898516109644616\n",
      "Test accuracy: 0.8305206298828125\n"
     ]
    }
   ],
   "source": [
    "model_d = Sequential()\n",
    "model_d.add(Embedding(10000, 32))\n",
    "model_d.add(LSTM(32, return_sequences=True))\n",
    "model_d.add(LSTM(32, return_sequences=True))\n",
    "model_d.add(LSTM(32))\n",
    "model_d.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model_d.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model_d.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model_d.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, I have three LSTM layers, we see that the accuracy of the model (83) decreases considerably from model C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: A model using an Embedding layer with bidirectional sequential layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 4s 3ms/step - loss: 0.6396 - acc: 0.7643 - val_loss: 0.5036 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.5000 - acc: 0.8000 - val_loss: 0.4976 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.4966 - acc: 0.8000 - val_loss: 0.4965 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.4946 - acc: 0.8000 - val_loss: 0.4954 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.4908 - acc: 0.8000 - val_loss: 0.4913 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.4815 - acc: 0.8000 - val_loss: 0.4832 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.4641 - acc: 0.8000 - val_loss: 0.4633 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.4300 - acc: 0.8015 - val_loss: 0.4139 - val_acc: 0.8006\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.3944 - acc: 0.8217 - val_loss: 0.4106 - val_acc: 0.8269\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.3484 - acc: 0.8531 - val_loss: 0.3345 - val_acc: 0.8557\n",
      "557/557 [==============================] - 1s 927us/step\n",
      "Test score: 0.349355810099487\n",
      "Test accuracy: 0.8542190194129944\n"
     ]
    }
   ],
   "source": [
    "model_e = Sequential()\n",
    "model_e.add(layers.Embedding(max_features, 32))\n",
    "model_e.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model_e.add(layers.Dense(5, activation='sigmoid'))\n",
    "\n",
    "model_e.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model_e.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "score, acc = model_e.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model we have one bidirectional sequential layer and results in the accuracy of 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the best model (Model C) with drop outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1334/1334 [==============================] - 3s 3ms/step - loss: 0.5446 - acc: 0.7774 - val_loss: 0.4985 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4996 - acc: 0.8000 - val_loss: 0.4976 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4977 - acc: 0.8000 - val_loss: 0.4967 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4931 - acc: 0.8000 - val_loss: 0.4912 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4818 - acc: 0.8000 - val_loss: 0.4805 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4610 - acc: 0.8000 - val_loss: 0.4561 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4276 - acc: 0.8021 - val_loss: 0.4204 - val_acc: 0.8012\n",
      "Epoch 8/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3874 - acc: 0.8264 - val_loss: 0.3868 - val_acc: 0.8299\n",
      "Epoch 9/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3490 - acc: 0.8574 - val_loss: 0.3713 - val_acc: 0.8491\n",
      "Epoch 10/10\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3077 - acc: 0.8846 - val_loss: 0.3195 - val_acc: 0.8808\n",
      "557/557 [==============================] - 0s 407us/step\n",
      "Test score: 0.3390718218561135\n",
      "Test accuracy: 0.8728905320167542\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=32)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining model C using a dropout leads to a decreased accuracy of 87%. Below I rerun the model using increased epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the best model (Model C) with increased epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qures\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1334 samples, validate on 334 samples\n",
      "Epoch 1/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.5435 - acc: 0.7825 - val_loss: 0.4972 - val_acc: 0.8000\n",
      "Epoch 2/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.5002 - acc: 0.8000 - val_loss: 0.4968 - val_acc: 0.8000\n",
      "Epoch 3/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4978 - acc: 0.8000 - val_loss: 0.4959 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4944 - acc: 0.8000 - val_loss: 0.4922 - val_acc: 0.8000\n",
      "Epoch 5/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4860 - acc: 0.8000 - val_loss: 0.4832 - val_acc: 0.8000\n",
      "Epoch 6/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4656 - acc: 0.8000 - val_loss: 0.4614 - val_acc: 0.8006\n",
      "Epoch 7/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.4276 - acc: 0.8106 - val_loss: 0.4102 - val_acc: 0.8234\n",
      "Epoch 8/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3870 - acc: 0.8270 - val_loss: 0.3786 - val_acc: 0.8353\n",
      "Epoch 9/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3486 - acc: 0.8472 - val_loss: 0.3497 - val_acc: 0.8593\n",
      "Epoch 10/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.3050 - acc: 0.8732 - val_loss: 0.3365 - val_acc: 0.8587\n",
      "Epoch 11/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2706 - acc: 0.8990 - val_loss: 0.2881 - val_acc: 0.8874\n",
      "Epoch 12/20\n",
      "1334/1334 [==============================] - ETA: 0s - loss: 0.2368 - acc: 0.919 - 3s 2ms/step - loss: 0.2366 - acc: 0.9192 - val_loss: 0.2728 - val_acc: 0.8880\n",
      "Epoch 13/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2080 - acc: 0.9315 - val_loss: 0.2542 - val_acc: 0.9024\n",
      "Epoch 14/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1831 - acc: 0.9447 - val_loss: 0.2358 - val_acc: 0.9102\n",
      "Epoch 15/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1560 - acc: 0.9577 - val_loss: 0.2154 - val_acc: 0.9204\n",
      "Epoch 16/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1277 - acc: 0.9691 - val_loss: 0.2018 - val_acc: 0.9281\n",
      "Epoch 17/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1080 - acc: 0.9721 - val_loss: 0.2241 - val_acc: 0.9156\n",
      "Epoch 18/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0939 - acc: 0.9786 - val_loss: 0.2051 - val_acc: 0.9299\n",
      "Epoch 19/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0755 - acc: 0.9832 - val_loss: 0.1973 - val_acc: 0.9228\n",
      "Epoch 20/20\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0651 - acc: 0.9853 - val_loss: 0.1801 - val_acc: 0.9341\n",
      "557/557 [==============================] - 0s 399us/step\n",
      "Test score: 0.2097818600264657\n",
      "Test accuracy: 0.9278275966644287\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing epochs helped to increase the accuracy to a 90.48% but it is still not equivalent to the 92.78% in the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model C, where one embedding and one LSTM sequential layer was used, performed the best. This is not what I had expected as I imagined adding more sequential layers would make the model perform better. \n",
    "\n",
    "However all the subsequent models; biderectional LSTM, multiple LSTM layers and dropouts made the model perform worse. This might be due to the fact that the dataset is not very large.I would like to try using Glove embeddings or GRU in order to see if a diffferent methodology would provide a better and more accurate model. I have seen that multiple Conv1D layers and multiple LSTM by themselves do not improve the model. Perhaps combining them would improve our predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
