{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Projects Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portfolio showcases a sample of the ML techniques that I have experience with. It is not an exhaustive representation of my skillset but does exemplify the more advanced models that I am comfortable building. \n",
    "\n",
    "Below you will find information about three projects, including a description of the data, the objective of the project, the motivation behind the models used as well as any limitations identified in the data structure or the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1: Predicting World Happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "UN's World Happiness data from 2019 tabulates a number of features that may be considered predictors of happiness. A series of classifier models is built that would predict the Happiness level of a country accurately. \n",
    "\n",
    "#### Data\n",
    "\n",
    "This project uses UN's World Happiness data (2019). This data is in the tabular form and records happiness level of 156 countries and provides several other features such as the name of the country, GDP per capita, and proxy scores for social support, healthy life expectancy, freedom to make life choices, generosity and perceptions of corruption. \n",
    "\n",
    "This data is merged with the names of a country's region using a publicly available [dataset](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv) to come up with a variable that depicts the geography of an observation but is not as diverse as the name of the individual country. Using regions rather than countries as a categorical variable helps to limit the categories in the dataset and may prove to be a valuable predictor in my models. 'Region' is the only categorical variable in the dataset and is One Hot Encoded prior to applying any of the analytical techniques. \n",
    "\n",
    "#### Overview of Analytical Techniques\n",
    "\n",
    "##### Feature Selection\n",
    "\n",
    "I ran feature selection algorithms to determine which, out of the seven features that make up the independent variables, are the best predictors of World Happiness. These methods are better suited for tabular data with a large number of independent variables as they allow one to narrow down the number of significant predictors. \n",
    "\n",
    "There are several feature selection methods (Recurssive Feature Elimination, using regression/classification methods along with SelectFromModel etc.). I decided to use a ranking of chi-squared statistics and linear support vector machines to figure out the top 3 and the top singular feature predictive of World Happiness, respectively. \n",
    "\n",
    "##### Predictive Models\n",
    "\n",
    "The three models used here are K Nearest Neighbors (KNN), Logistic Regression and Multi Layer Perceptron (MLP). \n",
    "\n",
    "KNN helps to determine the classification of an observation in the test data by finding its k nearest neighbors in the training data using Euclidean distances. It is a simple classification technique but can be a good first step. We can use grid search to determine the optimum number of neighbors (k) to compare the test observation with.\n",
    "\n",
    "Logistic Regression is generally preferred for binary classifers but can be used as a multi class method through the use of \"one vs rest\" or, in this case, the \"multinomial\" method in scikit learn. The model experiences some cross entropy loss as a result.\n",
    "\n",
    "Multilayer Perceptron is a convenient way to build a neural network. It uses fully connected layers. I used one with 5 hidden layers. \n",
    "\n",
    "#### Notes\n",
    "\n",
    "Due to the size of the data, neither of the models is expected to perform with a very high accuracy. Yet they can be ranked with respect to each other and can help to guide the next steps.\n",
    "\n",
    "UN World Happiness Project can be found [here](https://github.com/samreennq/Machine_Learning_Projects/tree/master/Project%201%20-%20UN%20World%20Happiness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: Brain Tumor diagnostic MRI Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Open source MRI image data available on Kaggle is used in this project. The images are marked as having a tumor ('yes') or not ('no'). Four learning models are created to classify the images. \n",
    "\n",
    "#### Data\n",
    "\n",
    "There are 155 images marked 'yes' and 98 mrked 'no'. Dummy coded y-labels are created. The data is then randomized and split in test and train datasets. Our training data contains 202 images. The images were resized to 224 x 224 and RBG filters. \n",
    "\n",
    "#### Overview of Analytical Techniques\n",
    "\n",
    "For this project I used neural network models of increasing complexity; starting from a simple neural network and ending with transfer learning through VGG16 and Resnet.\n",
    "\n",
    "##### Simpler Networks\n",
    "\n",
    "My first model is a neural network with four hidden layers followed by a convolutional 2D NN. The CNN allows for a large number of trainable parameters. As such, I expect higher accuracy from the CNN then the traditional NN. \n",
    "\n",
    "##### Transfer Learning\n",
    "\n",
    "Transfer Learning implies the use of models trained on much larger datasets on smaller data in the hopes of using their learning prowess, which would be difficult to replicate if trained on small data. I used VGG16 and Resnet for transfer learning. For the ease of computation on my machine I fixed the parameters of the models and made them non trainable.\n",
    "\n",
    "VGG16 uses multiple Conv2D layers as such I expected the VGG16 to yeild similar, or better, accuracy as the CNN model. But the similarity of structure along with a lack of trainable parameters presents a very different analysis from the simpler CNN model.\n",
    "\n",
    "Resnet is a deep and complex network and usually used for very large datasets. As such it may have it's limitations in the limited number of images I wanalyzed. I also used a largely non trainable model for the ease of complexity. \n",
    "\n",
    "#### Notes\n",
    "\n",
    "Deeper neural networks require larger data. As such, we may be able to get better results by adjusting our traditional CNN then using transfer learning. It is difficult to avoid overfitting in deep learning models when using smaller datasets and it may require complex modifications to make TL methods functional.  \n",
    "\n",
    "Brain Tumor Image Classification Project can be found [here](https://github.com/samreennq/Machine_Learning_Projects/tree/master/Project%202%20-%20Brain%20Images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3: BBC News Category Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "This project uses yet another type of data (text). I look at a series of BBC News stories and their classification into news categories. \n",
    "\n",
    "#### Data\n",
    "\n",
    "The Data consists of 2225 news stories and the categories they are classified under. There are five news categories *sports, business, politics, tech* and *entertainment*. Most of the stories are either sports or business related ith entertainment trailing the ranks. \n",
    "\n",
    "The data is preprocessed where each of the text is turned in to a sequence of numbers or 'tokenized'. I set the max legnth of the story to the first 100 words and limit the vicabulary to 10,000 most commonly occuring words. \n",
    "\n",
    "Moreover, dummies of the dependent variable are created in order to run the multi class models. The data is then split into test and train sets.\n",
    "\n",
    "#### Overview of Analytical Techniques\n",
    "\n",
    "A number of models are run here which can be divided in to two main categories, models that do not feature Long Short Term Memory (LSTM) and models that do. \n",
    "\n",
    "The first two models only use embedding layers (needed to vectorize the sequence of words) and either simple dense hidden layers or convolutional 1D and max pooling layers. \n",
    "\n",
    "The remaining models use different architectures of LSTMs, a recurrent neural network structure that is generally used for sequential data such as time series but has applications in sentiment/text analysis as well. Models with stacked LSTMs and bidirectional LSTMs (Long Short Memory models built on the original as well as the reverse directions of initial sequence) are also analyzed. \n",
    "\n",
    "Moreover, the best of the models (which, in this case is the one with single LSTM layer) is then optimized using dropuouts and varying the number of epochs. Dropout is a feature where connections in recurrent layers can be dropped in order to avoid overfitting. In order to make sure that we are not severely underfitting after dropouts are incorporated, I increased the number of epochs to counter any issues related to over/under fitting. \n",
    "\n",
    "#### Notes\n",
    "\n",
    "This project epitomizes the fact that simpler models are often superior. Adding a variety of complexities to the simple LSTM model does not improve its predictive capabilities.\n",
    "\n",
    "BBC News Classification Project can be found [here](https://github.com/samreennq/Machine_Learning_Projects/tree/master/Project%203%20-%20BBC%20News%20Category)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
